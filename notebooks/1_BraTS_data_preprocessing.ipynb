{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91dc2916",
   "metadata": {},
   "source": [
    "Code can be divided into a few parts....\n",
    "\n",
    "1. Combine \n",
    "2. Changing mask pixel values (labels) from 4 to 3 (as the original labels are 0, 1, 2, 4)\n",
    "3. Visualize\n",
    "\n",
    "\n",
    "https://pypi.org/project/nibabel/\n",
    "\n",
    "All BraTS multimodal scans are available as NIfTI files (.nii.gz) -> commonly used medical imaging format to store brain imagin data obtained using MRI and describe different MRI settings\n",
    "\n",
    "- T1    - T1-weighted, native image, sagittal or axial 2D acquisitions, with 1-6 mm slice thickness.\n",
    "- T1c   - T1-weighted, contrast-enhanced (Gadolinium) image, with 3D acquisition and 1 mm isotropic voxel size for most patients.\n",
    "- T2    - T2-weighted image, axial 2D acquisition, with 2-6 mm slice thickness.\n",
    "- FLAIR - T2-weighted FLAIR image, axial, coronal, or sagittal 2D acquisitions, 2-6 mm slice thickness.\n",
    "\n",
    "Note: Segmented file name in Folder 355 has a weird name. Rename it to match others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6befd388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import glob\n",
    "from tensorflow.keras.utils import to_categorical       # Since it's a multi-class semantic segmentation we need to convert images into categorical \n",
    "import matplotlib.pyplot as plt\n",
    "from tifffile import imwrite             # To save as tiff files \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler          # To scale all values\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d387939",
   "metadata": {},
   "source": [
    "### Getting an initial understanding of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc2989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 1: Load sample images and visualize\n",
    "#Includes, dividing each image by its max to scale them to [0,1]\n",
    "#Converting mask from float to uint8\n",
    "#Changing mask pixel values (labels) from 4 to 3 (as the original labels are 0, 1, 2, 4)\n",
    "#Visualize\n",
    "\n",
    "\n",
    "# View a few images\n",
    "\n",
    "# Note: Segmented file name in Folder 355 had a weird name. Renamed it to match others.\n",
    "\n",
    "TRAIN_DATASET_PATH = 'BraTS_2020_dataset/MICCAI_BraTS2020_TrainingData/'\n",
    "#VALIDATION_DATASET_PATH = 'BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed8e459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a single image as a numpy array\n",
    "test_image_flair = nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_355/BraTS20_Training_355_flair.nii').get_fdata()\n",
    "\n",
    "print(test_image_flair.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12532c4",
   "metadata": {},
   "source": [
    "### Normalize NIfTI images to values btw 0 to 1 using MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e6da09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalers are applied to 1D so let us reshape and then reshape back to original shape, i.e flattening then reforming\n",
    "test_image_flair = scaler.fit_transform(test_image_flair.reshape(-1, test_image_flair.shape[-1])).reshape(test_image_flair.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dbe376",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_t1 = nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_355/BraTS20_Training_355_t1.nii').get_fdata()\n",
    "\n",
    "test_image_t1 = scaler.fit_transform(test_image_t1.reshape(-1, test_image_t1.shape[-1])).reshape(test_image_t1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820d7608",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_t1ce = nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_355/BraTS20_Training_355_t1ce.nii').get_fdata()\n",
    "\n",
    "test_image_t1ce = scaler.fit_transform(test_image_t1ce.reshape(-1, test_image_t1ce.shape[-1])).reshape(test_image_t1ce.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327d6b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_t2 = nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_355/BraTS20_Training_355_t2.nii').get_fdata()\n",
    "\n",
    "test_image_t2 = scaler.fit_transform(test_image_t2.reshape(-1, test_image_t2.shape[-1])).reshape(test_image_t2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bcba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mask = nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_355/BraTS20_Training_355_seg.nii').get_fdata()\n",
    "\n",
    "# Convert mask to type o funsigned integer 8 coz all we have is values 0, 1, 2, 4\n",
    "test_mask = test_mask.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eb5a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(test_mask))             # 0, 1, 2, 4 (Need to re-encode to 0, 1, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969fce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassign mask values from 4 to 3\n",
    "test_mask[test_mask == 4] = 3\n",
    "\n",
    "print(np.unique(test_mask)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7249a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly selecting a slice of brain & plotting \n",
    "import random\n",
    "\n",
    "n_slice=random.randint(0, test_mask.shape[2])\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(231)\n",
    "plt.imshow(test_image_flair[:,:,n_slice], cmap='gray')\n",
    "plt.title('Image flair')\n",
    "\n",
    "plt.subplot(232)\n",
    "plt.imshow(test_image_t1[:,:,n_slice], cmap='gray')\n",
    "plt.title('Image t1')\n",
    "\n",
    "plt.subplot(233)\n",
    "plt.imshow(test_image_t1ce[:,:,n_slice], cmap='gray')\n",
    "plt.title('Image t1ce')\n",
    "\n",
    "plt.subplot(234)\n",
    "plt.imshow(test_image_t2[:,:,n_slice], cmap='gray')\n",
    "plt.title('Image t2')\n",
    "\n",
    "plt.subplot(235)\n",
    "plt.imshow(test_mask[:,:,n_slice])\n",
    "plt.title('Mask')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# rerun this cell till u get satisfacory images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8b02aa",
   "metadata": {},
   "source": [
    "### Combine t1ce, t2, and flair into single numpy array (multichannel image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed85421b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 2: Explore the process of combining images to channels and divide them to patches\n",
    "# Includes...\n",
    "# Combining all 4 images to 4 channels of a numpy array so that we can train it later\n",
    "# Flair, T1CE, annd T2 have the most information\n",
    "\n",
    "combined_x = np.stack([test_image_flair, test_image_t1ce, test_image_t2], axis=3)\n",
    "\n",
    "# t1 ko bhi add karke test kar sakte hai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4a8a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop to a size to be divisible by 64 so we can later extract 64x64x64 patches. \n",
    "# cropping x, y, and z\n",
    "# combined_x = combined_x[24:216, 24:216, 13:141]\n",
    "\n",
    "combined_x = combined_x[56:184, 56:184, 13:141] #Crop to 128x128x128x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c21734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for mask\n",
    "test_mask = test_mask[56:184, 56:184, 13:141]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b6c97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "n_slice = random.randint(0, test_mask.shape[2])\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.imshow(combined_x[:,:,n_slice, 0], cmap='gray')\n",
    "plt.title('Image flair')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.imshow(combined_x[:,:,n_slice, 1], cmap='gray')\n",
    "plt.title('Image t1ce')\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.imshow(combined_x[:,:,n_slice, 2], cmap='gray')\n",
    "plt.title('Image t2')\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.imshow(test_mask[:,:,n_slice])\n",
    "plt.title('Mask')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b9509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imsave('BraTS2020_TrainingData/combined255.tif', combined_x)\n",
    "\n",
    "# Instead of saving as a tiff file, prefer ot save as a numpy array\n",
    "\n",
    "np.save('BraTS2020_TrainingData/combined255.npy', combined_x)\n",
    "\n",
    "# Verify image is being read properly\n",
    "# my_img=imread('BraTS_2020_dataset/combined255.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a68ad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "my_img = np.load('BraTS2020_TrainingData/combined255.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338511fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert mask to categorical\n",
    "test_mask = to_categorical(test_mask, num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39a4dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let us apply the same as above to all the images...\n",
    "# Merge channels, crop, patchify, save\n",
    "# GET DATA READY =  GENERATORS OR OTHERWISE\n",
    "\n",
    "# Keras datagenerator does not support 3d\n",
    "\n",
    "\n",
    "# images lists harley\n",
    "\n",
    "#t1_list = sorted(glob.glob('BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/*/*t1.nii'))              # t1_list is not req\n",
    "\n",
    "t2_list = sorted(glob.glob('BraTS_2020_dataset/MICCAI_BraTS2020_TrainingData/*/*t2.nii'))\n",
    "\n",
    "t1ce_list = sorted(glob.glob('BraTS_2020_dataset/MICCAI_BraTS2020_TrainingData/*/*t1ce.nii'))\n",
    "\n",
    "flair_list = sorted(glob.glob('BraTS_2020_dataset/MICCAI_BraTS2020_TrainingData/*/*flair.nii'))\n",
    "\n",
    "mask_list = sorted(glob.glob('BraTS_2020_dataset/MICCAI_BraTS2020_TrainingData/*/*seg.nii'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b359e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each volume generates 18 64x64x64x4 sub-volumes. \n",
    "# Total 369 volumes = 6642 sub volumes\n",
    "\n",
    "for img in range(len(t2_list)):             # Using t1_list as all lists are of same size\n",
    "    \n",
    "    print(\"Now preparing image and masks number: \", img)\n",
    "      \n",
    "    temp_image_t2 = nib.load(t2_list[img]).get_fdata()\n",
    "    temp_image_t2 = scaler.fit_transform(temp_image_t2.reshape(-1, temp_image_t2.shape[-1])).reshape(temp_image_t2.shape)\n",
    "   \n",
    "    temp_image_t1ce = nib.load(t1ce_list[img]).get_fdata()\n",
    "    temp_image_t1ce = scaler.fit_transform(temp_image_t1ce.reshape(-1, temp_image_t1ce.shape[-1])).reshape(temp_image_t1ce.shape)\n",
    "   \n",
    "    temp_image_flair = nib.load(flair_list[img]).get_fdata()\n",
    "    temp_image_flair = scaler.fit_transform(temp_image_flair.reshape(-1, temp_image_flair.shape[-1])).reshape(temp_image_flair.shape)\n",
    "        \n",
    "\n",
    "    temp_mask = nib.load(mask_list[img]).get_fdata()\n",
    "    temp_mask = temp_mask.astype(np.uint8)\n",
    "    temp_mask[temp_mask == 4] = 3         # Reassign mask values 4 to 3\n",
    "    # print(np.unique(temp_mask))\n",
    "    \n",
    "    \n",
    "    # Stack flair, t1ce & t2 together\n",
    "    temp_combined_images = np.stack([temp_image_flair, temp_image_t1ce, temp_image_t2], axis = 3)\n",
    "    \n",
    "    # Crop to a size to be divisible by 64 so we can later extract 64x64x64 patches. \n",
    "    # cropping x, y, and z\n",
    "    temp_combined_images=temp_combined_images[56:184, 56:184, 13:141]\n",
    "    temp_mask = temp_mask[56:184, 56:184, 13:141]\n",
    "    \n",
    "    \n",
    "    # Finding volumes with segmented vol > 1%\n",
    "    \n",
    "    val, counts = np.unique(temp_mask, return_counts=True)\n",
    "    \n",
    "    # At least 1% useful volume with labels that are not 0\n",
    "    if (1 - (counts[0]/counts.sum())) > 0.01:  \n",
    "\n",
    "        print(\"Save Me\")\n",
    "        temp_mask= to_categorical(temp_mask, \n",
    "        num_classes=4)\n",
    "        \n",
    "        np.save('BraTS_2020_dataset/input_data_3channels/images/image_' + str(img)+'.npy', temp_combined_images)\n",
    "        \n",
    "        np.save('BraTS_2020_dataset/input_data_3channels/masks/mask_' + str(img) + '.npy', temp_mask)\n",
    "        \n",
    "    else:\n",
    "        print(\"I am useless\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4e5c33",
   "metadata": {},
   "source": [
    "Jab tu code run kar rha hoga toh instead of splitting training data into train, test & val, use the separate validation folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caddd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the same from above for validation data folder (prefer this) OR Split training data into train and validation (not this, although DigitalSreeni did this for faster training of model)\n",
    "\n",
    "\"\"\"\n",
    "Code for splitting folder into train, test, and val.\n",
    "Once the new folders are created rename them and arrange in the format below to be used\n",
    "for semantic segmentation using data generators. \n",
    "\n",
    "pip install split-folders\n",
    "\"\"\"\n",
    "# import splitfolders         # or import split_folders\n",
    "\n",
    "# input_folder = 'BraTS2020_TrainingData/input_data_3channels/'\n",
    "# output_folder = 'BraTS2020_TrainingData/input_data_128/'\n",
    "\n",
    "# # Split with a ratio.\n",
    "# # To only split into training and validation set, set a tuple to `ratio`, i.e, `(.8, .2)`.\n",
    "# splitfolders.ratio(input_folder, output=output_folder, seed=42, ratio=(.75, .25), group_prefix=None) # default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2f0344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
